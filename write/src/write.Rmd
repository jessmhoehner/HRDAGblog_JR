---
title: 'From Scripts to Projects: Using a Modular, Auditable, and Reproducible Workflow to Investigate a Potential Epidemic'
author: "Jessica Randall"
date: "12/29/2019"
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction {-}

Having been a big fan of HRDAG's work and fellow alumna of Emory's Rollin's School of Public Health, I have been delighted to have had opportunities to chat with both Megan and Patrick over the last year. I was equally thrilled when Patrick asked if I might want to work on an epidemiological problem the team had been presented with. While I had read about the organization's modular workflow (link) and watched Patrick's talk on how to do principled data processing (link), a lot of it was initially beyond my scope as someone who had only formally learned SAS and dabbled in self-taught Python and R. I fully admit to my R code looking suspiciously similar to all of the examples of amateur coding that Tarak uses in his post on why .Rproj is harmful to reproducible and auditable data science (link). 

Excited to help out where I could and learn from a master, I was undettered by the prospect of a crash course in the HRDAG process necessitated by the inherent immediacy of the need for an answer; were the data suggestive of an epidemic or not? To that end and with a great deal of Patrick's guidance I began to shift towards working mainly in the command line as opposed to R Studio and to start thinking of scripts in terms of a larger project architecture as opposed to one-off bits of code that I'd write per project. 

As I worked, Patrick also encouraged me to keep notes on what I liked and disliked about the process, where I felt it made things easier and where I noticed friction. The following post summarizes those notes and the work that went into determining the answers to the client's question. 

## Setup {-}

To begin the project, we start with a task. Each task is a microchasm of the entire project and at HGRAG, not only does this serve to facilitate the team's collaboration across time zones and programming languages. This results in projects which are auditable and have reproducible results; anyone looking at the code can determine exactly how any output from it was produced and anyone working on the task at anytime can pick it up and recreate the same output you wrote. As a scientist, this need for reproducibility resonated with me and even if you are a team of one in your workplace (as many statisticians out there might be) your most important collaborator is you six months from now. Break up your project into discrete tasks and thank yourself later.

To make each task even clearer, each task has a Makefile. The Makefile serves as an outline of each task not only for our benefit, but for the computer to read as well. Imagine being able to work on a group project with team members around the world, and still have an auditable trail of each person's work that just works. For more information on Make files check out Mike Bostock's post here (link) and for the inspiration for HRDAG's project oriented workflow check out Jenny Bryan's post here (link). 

For this project I have a clean task, a test task, and a write task each with its own Makefile, input, output, and src folder for the source code. 

## Clean {-}

I started with the clean task. In the input folder of this task I include the two datasets of cases of the potential epidemic, the src folder contains the R code to clean up the names of the variables. I used RStudio to test the code for bugs (with frequent restarts and fresh environments) and use Visual Studio Code to update the code that gets pushed to the project Github repository. I had a small bug early on with an error message I didn't want to immediately address so I created an issue on the GitHub to pick it back up later. I knew where the code had issues but I could count on the rest of it working as expected if (when) I'd need to spend time on other projects. This also had the added benefit (trap?) of making me feel like I'd actually made progress instead of staring down the endless empty field of .R file with however many tasks left to code. Win!

#Would it be better to just make these one file in the clean task or show the initial results as we saw them in two different sets and why we thought we needed more evidence before coming to a conclusion? 
#I think it's valuable to show that process but it may not be super relevant to the idea of the workflow

The first file includes initial cases reported from 12 June 2019 -- 10 September 2019. The second file contains additional cases reported from 1 January 2019 -- 14 December 2019. Our client would like us to determine whether or not the pattern of these cases suggests an epidemic of #some outcome of interest, using flu as a placeholder#. First, we'd like to do some exploratory data analysis and summarize how much complete information we have for each individual reported on in the dataset.

To do this, we pull in the clean data from the clean task's output folder and examine mising data using the DataExplorer package. At HRDAG, everybody counts, and since we assume that the data we are presented with is incomplete, we'd like to get an idea of where exactly information is not recorded.

```{r, clean, message=FALSE}
require (pacman)
p_load(dplyr,styler,tidyverse,forcats,readr,janitor,assertr, ggplot2, scales, DataExplorer)
require(here, assertr)

files <- list(input1=here::here("clean/input/flu_data_1_122219.txt"),
              input2=here::here("clean/input/flu_data_2_122219.txt"),
              output1=here::here("clean/output/flu1_clean.txt"),
              output2=here::here("clean/output/flu2_clean.txt"))
stopifnot(is.list(files)== TRUE)

#want to add in variables which are counts of the number of times each date occurs called cases/day here or do it just before graphing?

flu1 <- readr::read_delim(files$input1, delim="|") %>%
  clean_names() %>%
  mutate(age = as.integer(age)) %>%
  mutate(DOD = as.Date(`date`, "%m/%d/%Y")) %>%
  mutate_at(vars(DOD,sex,status), as.factor)
write_delim(flu1, files$output1, delim="|", na= "NA")
stopifnot(ncol(flu1) == 6 & (nrow (flu1) == 108))

flu2 <- readr::read_delim(files$input2, delim="|") %>%
  clean_names() %>%
  mutate(age = as.integer(age)) %>%
  mutate(DOD = as.Date(`date`, "%m/%d/%Y")) %>%
  mutate_at(vars(DOD,sex,status), as.factor)
write_delim(flu2,files$output2, delim="|", na= "NA")
stopifnot(ncol(flu2) == 6 & (nrow (flu2) == 974))

plot_missing(flu1)

plot_missing(flu2)

```
## Summarize variables {-}

#should this be its own task where we create and call the summary information from tables?
First Set

DOD
There are some days where we see (max) cases reported per day. In order to determine if this is an epidemic, we must examine an epidemiological curve to see if this number of cases per day appears to be significantly greater than the average cases reported each day.

Status
It looks like we have some people recorded who are not cases, since we are only interested in the number of cases over time, we will remove all rows where the person's status is "Not_flu".

sex
There are `r table(flu1$sex)[['F']]` Females and `r table(flu1$sex)[['M']]` Males in the first set with an overall mean age of `r round(mean(flu1$age, na.rm=T), 0)` and a median age of `r quantile(flu1$age, probs=c(0.5), na.rm=TRUE)[[1]]`. 

time
Most cases were reported between (earliest time) (latest time). 0.93% of people in this dataset are missing information on the time at which their case was reported 

age 
The youngest reported individual is `r min(flu1$age, na.rm=T)` and the oldest reported individual is reported as `r max(flu1$age, na.rm=T)`, with `r sum(is.na(flu1$age))`or 6.48% of individuals without age data. 


Second Set
DOD
There are some days where we see (max) cases reported per day. Again, we will an epidemiological curve to see if this number of cases per day appears to be significantly greater than the average cases reported each day.

Status
Here we have many more people recorded who are not cases and again we will remove all rows where the person's status is "Not_flu".

sex
There are `r table(flu2$sex)[['F']]` Females and `r table(flu2$sex)[['M']]` Males in the first set with an overall mean age of `r round(mean(flu2$age, na.rm=T), 0)` and a median age of `r quantile(flu2$age, probs=c(0.5), na.rm=TRUE)[[1]]`. 

time
Most cases were reported between (earliest time) (latest time).

age 
The youngest reported individual is `r min(flu2$age, na.rm=T)` and the oldest reported individual is reported as `r max(flu2$age, na.rm=T)`, with `r sum(is.na(flu2$age))`or 7.49% of individuals without age data. 

Fortunately there appears to be very little missing data in the dates, this will be critical in setting up our epidemiological curves.

## Daily Epidemic Curve (All Reported Cases) From 1st Dataset {-}
The first set includes initial cases reported from 12 June 2019 -- 10 September 2019. The second set contains additional cases reported from 1 January 2019 -- 14 December 2019. Our client would like us to determine whether or not the pattern of these cases suggests an epidemic of #some outcome of interest, using flu as a placeholder# and to do that we will need to examine them using a common epidemiological tool, an epidemiological curve. 

In order to examine the frequency of reported cases per day across the time period provided, I use the EpiCurve function to specify the dates, counts of the event of interest and the period at which I would like to see each bar. Here

```{r epic_DOD1, message=FALSE, echo=FALSE}
#remove notflu cases
flu1 <- filter(flu1, status == "flu")

flu1$DOD<- as.Date(flu1$DOD)
stopifnot(is.Date(flu1$DOD))

# compute daily incidence
date_case1<- incidence(flu1$DOD, interval = 1 )

epiplot1<-
  plot(date_case1, border = "white", show_cases=TRUE, 
       ylab = "Reported Cases", xlab= "Date") +
  scale_x_date(labels = date_format("%d %b %Y"))
  theme_grey() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, color = "black")) +
  coord_fixed(ratio=2)

epiplot1
```
This pattern appears to suggest that cases increased in the beginning of August but since these are only from approximately 100 people, we were unsure as to how to evaluate whether these were outside of what we would expect to see within this population. 

Let's examine the 2nd set of data with around 900 people, first removing those people who are not cases, leaving us with nrows(flu2) cases over a period of approximately 1 year. 

```{r epic_DOD2, message=FALSE, echo=FALSE}
#remove notflu cases
flu2 <- filter(flu2, status == "flu")

flu2$DOD<- as.Date(flu2$DOD)
stopifnot(is.Date(flu2$DOD))

# compute daily incidence
date_case2<- incidence(flu2$DOD, interval = 1 )

epiplot2<-
  plot(date_case2, border = "white", show_cases=TRUE, 
       ylab = "Reported Cases", xlab= "Date") +
  theme_grey() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, color = "black")) +
  coord_fixed(ratio=2)

epiplot2
```

As is common with data in the human rights context, it can often be difficult to obtain official statistics on incidence of a particular outcome of interest. Some group may have an interest in that data not existing in one central location or it may simply never have been collected. Either way, this requires us to break from traditional epidemiological methods and return instead to a classical statistical approach.

These cases are counts. This means that they can be assessed in the context of a Poisson distribution. Data which follow a normal Posisson distribution exhibit expected amounts of variation. In order to establish whether or not these data suggest an epidemic we construct our hypothesis to ask: are there more cases occuring than we would expect to see within a Poisson distribution? Specifically, on the days when we are seeing 4 and 5 cases per day, are these numbers statistically significantly greater than we would expect with normal variation or are they high but not especially so? 

If the number of days on which 4 and 5 cases are reported (or, the frequency of the days ) is #stopped on 12.29.19 2:34AM

## Daily Epidemic Curve (All Reported Cases) From 2nd Dataset {-}

Here we examine the epi curve of the second dataset which includes data from ~600 individuals who have reported as cases.

```{r epic_DOD, message=FALSE, echo=FALSE}
flu2$DOD<- as.Date(flu2$DOD)
assert_that(is.Date(flu2$DOD))

# compute daily incidence
date_case<- incidence(flu2$DOD, interval = 1 )

epiplot<-
  plot(date_case, border = "white", show_cases=TRUE, 
       ylab = "Reported Cases", xlab= "Date") +
  theme_grey() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, color = "black")) +
  coord_fixed(ratio=2)

epiplot

DODarray<-as.array(flu2$DOD, na.rm=FALSE)
table(DODarray)

ggsave("epic_DOD.png", plot=last_plot(), dpi= 600)
```
Does this suggest an unusual number of cases at any one point in time?

## Hypothesis testing task


## Conclusion {-}


```{r session_info, message=FALSE, include=FALSE}
sessionInfo()
```
<!---- done ---->
