---
title: 'From Scripts to Projects: Using a Modular, Auditable, and Reproducible Workflow to Investigate a Potential Epidemic'
author: "Jessica Randall"
date: "12/29/2019"
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction {-}

Having been a big fan of HRDAG's work and fellow alumna of Emory's Rollin's School of Public Health, I have been delighted to have had opportunities to chat with both Megan and Patrick over the last year. I was equally thrilled when Patrick asked if I might want to work on an epidemiological problem the team had been presented with. While I had read about the organization's modular workflow (link) and watched Patrick's talk on how to do principled data processing (link), a lot of it was initially beyond my scope as someone who had only formally learned SAS and dabbled in self-taught Python and R. I fully admit to my R code looking suspiciously similar to all of the examples of amateur coding that Tarak uses in his post on why .Rproj is harmful to reproducible and auditable data science (link). 

Excited to help out where I could and learn from a master, I was undettered by the prospect of a crash course in the HRDAG process necessitated by the inherent immediacy of the need for an answer; were the data suggestive of an epidemic or not? To that end and with a great deal of Patrick's guidance I began to shift towards working mainly in the command line as opposed to R Studio and to start thinking of scripts in terms of a larger project architecture as opposed to one-off bits of code that I'd write per project. 

As I worked, Patrick also encouraged me to keep notes on what I liked and disliked about the process, where I felt it made things easier and where I noticed friction. The following post summarizes those notes and the work that went into determining the answers to the client's question. 

## Setup and Import {-}

To begin the project, we start with a task. Each task is a microchasm of the entire project and at HGRAG, not only does this serve to facilitate the team's collaboration across time zones and programming languages. This results in projects which are auditable and have reproducible results; anyone looking at the code can determine exactly how any output from it was produced and anyone working on the task at anytime can pick it up and recreate the same output you wrote. As a scientist, this need for reproducibility resonated with me and even if you are a team of one in your workplace (as many statisticians out there might be) your most important collaborator is you six months from now. Break up your project into discrete tasks and thank yourself later.

To make each task even clearer, each task has a Makefile. The Makefile serves as an outline of each task not only for our benefit, but for the computer to read as well. Imagine being able to work on a group project with team members around the world, and still have an auditable trail of each person's work that just works. For more information on Make files check out Mike Bostock's post here (link) and for the inspiration for HRDAG's project oriented workflow check out Jenny Bryan's post here (link). 

For this project I have an import task, a clean task, and a write task each with its own Makefile, input, outout, and src folder for the source code. 
I started with the import task. In the input folder I include the two files I'd like to examine. These are the two datasets with cases of the potential epidemic. 
Next, the clean task takes the data from the import folder's input folder, runs the code in the clean task on it, and puts the cleaned data into the clean task's output folder.

#Would it be better to just make these one file in the clean task or show the initial results as we saw them in two different sets and why we thought we needed more evidence before coming to a conclusion? 
#I think it's valuable to show that process but it may not be super relevant to the idea of the workflow

The first file includes initial cases reported from 12 June 2019 -- 10 September 2019. The second file contains additional cases reported from 1 January 2019 -- 14 December 2019. Our client would like us to determine whether or not the pattern of these cases suggests an epidemic of #some outcome of interest, using flu as a placeholder# and to do that we will need to examine them using a common epidemiological tool, an epidemiological curve. 


```{r, import, message=FALSE}

pacman::p_load("dplyr", "styler", "tidyverse", "tidytext", "lubridate", "tinytex", "here", "readr", "janitor", "incidence", "epitools", "assertthat", "DataExplorer")
              
files <- list(input=here::here("import/input/flu_data_1_122219.csv"),
			        input2=here::here("import/input/flu_data_2_122219.rds"),
              output=here::here("clean/output/flu1.csv"),
			        output2=here::here("clean/output/flu2.rds"))
#test
stopifnot(is.list(files)== TRUE)

flu1 <- readr::read_delim(files$cleanflu1, delim=",")
stopifnot(nrow(flu1) == 109) #break it first, 107 breaks it

flu2 <- readr::read_delim(files$cleanflu2, delim=",")
stopifnot(nrow(flu1) == 975) #break it first, 900 breaks it
```

## Clean data {-}

In order to make this data usable I have set all of the values of each variable which are either empty, only contain a period, or contain NA all as missing. The date information is set as Year--Month--Day, Age is set as numeric, Time is a factor and Status are factors.

```{r clean, echo=FALSE,}

```
## Summarize variables {-}

There are `r nrow(flu1)` rows indicating individual cases and `r ncol(flu1)` columns indicating information about the individual and circumstances surrounding their cases. There are `r table(flu1$sex)[['F']]` Females and `r table(flu1$sex)[['M']]` Males in this flu1 set with an overall mean age of `r round(mean(flu1$age, na.rm=T), 0)` and a median age of `r quantile(flu1$age, probs=c(0.5), na.rm=TRUE)[[1]]`. The youngest reported individual is `r min(flu1$age, na.rm=T)` and the oldest reported individual is reported as `r max(flu1$age, na.rm=T)`, with `r sum(is.na(flu1$age))` individuals without age data.

The date with the greatest frequency of reported cases `r mode(flu1$DOD)` cases was 7 September 2019, followed by 5 cases reported on 5 September and 4 cases each reported on both 6 September and 15 June 2019. The most frequent reported time of case was `r mode(flu1$TOD)`, with `r mode(flu1$TOD)` individuals being reported dead at this time and 5 individuals each reported dead at the hours of 5:00 AM, 7:00 AM and 12:00 AM.

The Area with the highest frequency of cases is reported to be `r mode(flu1$area)` (8 individuals) followed by 7 individuals reported dead in Wong Tai Sin and 6 individuals each reported dead in Kwun Tong and Shatin.

`r mode(flu1$incident_type["Fall from bldg"])` individuals were reported as having fallen from a building with the second most frequent incident type listed as "hanging". `r table(flu1$suicide_note_found['N'])` individuals were reported to have not left a suicide note, `r table(flu1$suicide_note_found['Y'])` were reported to have left a suicide note and `r table(flu1$suicide_note_found['NA'])` individuals do not have information for this variable.


## Daily Epidemic Curve (All Reported Cases) From 1st Dataset {-}

In order to examine the frequency of reported cases per day across the time period provided, I use the EpiCurve function to specify the dates, counts of the event of interest and the period at which I would like to see each bar. Here

```{r epic_DOD, message=FALSE, echo=FALSE}
flu1$DOD<- as.Date(flu1$DOD)
assert_that(is.Date(flu1$DOD))

# compute daily incidence
date_case<- incidence(flu1$DOD, interval = 1 )

epiplot<-
  plot(date_case, border = "white", show_cases=TRUE, 
       ylab = "Reported Cases", xlab= "Date") +
  theme_grey() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, color = "black")) +
  coord_fixed(ratio=2)

epiplot

DODarray<-as.array(flu1$DOD, na.rm=FALSE)
table(DODarray)

ggsave("epic_DOD.png", plot=last_plot(), dpi= 600)
```
This pattern appeared to us to us that cases increased in the beginning of August but since these are only from approximately 100 people, we were unsure as to how to evaluate whether these were outside of what we would expect to see within this population. As is common with data in the human rights context, it can often be difficult to obtain official statistics on incidence of a particular outcome of interest. TSome group may have an interest in that data not existing in one central location or it may simply never have been collected. Either way, this requires us to break from traditional epidemiological methods and return instead to a classical statistical approach. 

These cases are counts. This means that they can be assessed in the context of a Poisson distribution. Data which follow a normal Posisson distribution exhibit expected amounts of variation. In order to establish whether or not these data suggest an epidemic we construct our hypothesis to ask: are there more cases occuring than we would expect to see within a Poisson distribution? Specifically, on the days when we are seeing 4 and 5 cases per day, are these numbers statistically significantly greater than we would expect with normal variation or are they high but not especially so? 

If the number of days on which 4 and 5 cases are reported (or, the frequency of the days ) is #stopped on 12.29.19 2:34AM

## Daily Epidemic Curve (All Reported Cases) From 2nd Dataset {-}

Here we examine the epi curve of the second dataset which includes data from ~600 individuals who have reported as cases.

```{r epic_DOD, message=FALSE, echo=FALSE}
flu2$DOD<- as.Date(flu2$DOD)
assert_that(is.Date(flu2$DOD))

# compute daily incidence
date_case<- incidence(flu2$DOD, interval = 1 )

epiplot<-
  plot(date_case, border = "white", show_cases=TRUE, 
       ylab = "Reported Cases", xlab= "Date") +
  theme_grey() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, color = "black")) +
  coord_fixed(ratio=2)

epiplot

DODarray<-as.array(flu2$DOD, na.rm=FALSE)
table(DODarray)

ggsave("epic_DOD.png", plot=last_plot(), dpi= 600)
```
Does this suggest an unusual number of cases at any one point in time?

## Hypothesis testing task


## Conclusion {-}


```{r session_info, message=FALSE, include=FALSE}
sessionInfo()
```
<!---- done ---->
