---
title: 'From Scripts to Projects: Using a Modular, Auditable, and Reproducible Workflow to Investigate a Potential Epidemic'
author: "Jessica Randall"
date: "12/29/2019"
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction {-}

Having been a big fan of HRDAG's work and fellow alumna of Emory's Rollin's School of Public Health, I have been delighted to have had opportunities to chat with both Megan and Patrick over the last year. I was equally thrilled when Patrick asked if I might want to work on an epidemiological problem the team had been presented with. While I had read about the organization's modular workflow (link) and watched Patrick's talk on how to do principled data processing (link), a lot of it was initially beyond my scope as someone who had only formally learned SAS and dabbled in self-taught Python and R. I fully admit to my R code looking suspiciously similar to all of the examples of amateur coding that Tarak uses in his post on why .Rproj is harmful to reproducible and auditable data science (link). 

Excited to help out where I could and learn from a master, I was undettered by the prospect of a crash course in the HRDAG process necessitated by the inherent immediacy of the need for an answer; were the data suggestive of an epidemic or not? To that end and with a great deal of Patrick's guidance I began to shift towards working mainly in the command line as opposed to R Studio and to start thinking of scripts in terms of a larger project architecture as opposed to one-off bits of code that I'd write per project. 

As I worked, Patrick also encouraged me to keep notes on what I liked and disliked about the process, where I felt it made things easier and where I noticed friction. The following post summarizes those notes and the work that went into determining the answers to the client's question. 

## Setup {-}

To begin the project, we start with a task. Each task is a microchasm of the entire project and at HGRAG, not only does this serve to facilitate the team's collaboration across time zones and programming languages. This results in projects which are auditable and have reproducible results; anyone looking at the code can determine exactly how any output from it was produced and anyone working on the task at anytime can pick it up and recreate the same output you wrote. As a scientist, this need for reproducibility resonated with me and even if you are a team of one in your workplace (as many statisticians out there might be) your most important collaborator is you six months from now. Break up your project into discrete tasks and thank yourself later.

To make each task even clearer, each task has a Makefile. The Makefile serves as an outline of each task not only for our benefit, but for the computer to read as well. Imagine being able to work on a group project with team members around the world, and still have an auditable trail of each person's work that just works. For more information on Make files check out Mike Bostock's post here (link) and for the inspiration for HRDAG's project oriented workflow check out Jenny Bryan's post here (link). 

For this project I have a clean task, a test task, and a write task each with its own Makefile, input, output, and src folder for the source code. 

## Clean {-}

I started with the clean task. In the input folder of this task I include the two datasets of cases of the potential epidemic, the src folder contains the R code to clean up the names of the variables. I used RStudio to test the code for bugs (with frequent restarts and fresh environments) and use Visual Studio Code to update the code that gets pushed to the project Github repository. I had a small bug early on with an error message I didn't want to immediately address so I created an issue on the GitHub to pick it back up later. I knew where the code had issues but I could count on the rest of it working as expected if (when) I'd need to spend time on other projects. This also had the added benefit (trap?) of making me feel like I'd actually made progress instead of staring down the endless empty field of .R file with however many tasks left to code. Win!

#Would it be better to just make these one file in the clean task or show the initial results as we saw them in two different sets and why we thought we needed more evidence before coming to a conclusion? 
#I think it's valuable to show that process but it may not be super relevant to the idea of the workflow

The first file includes initial cases reported from 12 June 2019 -- 10 September 2019. The second file contains additional cases reported from 1 January 2019 -- 14 December 2019. Our client would like us to determine whether or not the pattern of these cases suggests an epidemic of #some outcome of interest, using flu as a placeholder#. First, we'd like to do some exploratory data analysis and summarize how much complete information we have for each individual reported on in the dataset.

To do this, we pull in the clean data from the clean task's output folder and examine mising data using the DataExplorer package. At HRDAG, everybody counts, and since we assume that the data we are presented with is incomplete, we'd like to get an idea of where exactly information is not recorded.

```{r, clean, message=FALSE}
require (pacman)
pacman::p_load(dplyr,styler,tidyverse,
               forcats,readr,janitor,
               assertr, ggplot2, lubridate,
               scales, DataExplorer, incidence,
               exactci)

files <- list(input1=here::here("clean/input/flu_data_1_122219.txt"),
              input2=here::here("clean/input/flu_data_2_122219.txt"),
              output1=here::here("clean/output/flu1_clean.txt"),
              output2=here::here("clean/output/flu2_clean.txt"))
stopifnot(is.list(files)== TRUE)

#want to add in variables which are counts of the number of times each date occurs called cases/day here or do it just before graphing?

flu1 <- readr::read_delim(files$input1, delim="|") %>%
  clean_names() %>%
  mutate(age = as.integer(age)) %>%
  mutate(DOD = as.Date(`date`, "%m/%d/%Y")) %>%
  mutate_at(vars(DOD,sex,status), as.factor)
write_delim(flu1, files$output1, delim="|", na= "NA")
stopifnot(ncol(flu1) == 6 & (nrow (flu1) == 108))

flu2 <- readr::read_delim(files$input2, delim="|") %>%
  clean_names() %>%
  mutate(age = as.integer(age)) %>%
  mutate(DOD = as.Date(`date`, "%m/%d/%Y")) %>%
  mutate_at(vars(DOD,sex,status), as.factor)
write_delim(flu2,files$output2, delim="|", na= "NA")
stopifnot(ncol(flu2) == 6 & (nrow (flu2) == 974))

plot_missing(flu1)

plot_missing(flu2)

```
## Summarize variables {-}

We define a function that will allow us to report the most frequently occuring date of report in each of the datasets and call it getmode.

```{r, summarize, message=FALSE}

getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

```

#should this be its own task where we create and call the summary information from tables?

In order to determine if this is an epidemic, we must examine an epidemiological curve to see if this number of cases per day appears to be significantly greater than the average cases reported each day. The day on which most cases were reported in the first set was `r getmode(flu1$DOD)` and in the second set was `r getmode(flu2$DOD)`

It looks like we have some people recorded who are not cases, since we are only interested in the number of cases over time, we will remove all rows where the person's status is "Not_flu". For the first set we'll be removing `r table(flu1$status)[["Not_flu"]]` people and in the second set we'll be removing `r table(flu2$status)[["Not_flu"]]` people.

In the first set there are `r table(flu1$sex)[["F"]]` Females and `r table(flu1$sex)[["M"]]` Males with overall mean age of `r round(mean(flu1$age, na.rm=T), 0)` and a median age of `r quantile(flu1$age, probs=c(0.5), na.rm=TRUE)[[1]]`. In the second set there are `r table(flu2$sex)[["F"]]` Females and `r table(flu2$sex)[["M"]]` Males with overall mean age of `r round(mean(flu2$age, na.rm=T), 0)` and a median age of `r quantile(flu2$age, probs=c(0.5), na.rm=TRUE)[[1]]`. 

Fortunately there appears to be very little missing data in the dates, this will be critical in setting up our epidemiological curves.

## Daily Epidemic Curve (All Reported Cases) From 1st Dataset {-}
The first set includes initial cases reported from 12 June 2019 -- 10 September 2019. The second set contains additional cases reported from 1 January 2019 -- 14 December 2019. Our client would like us to determine whether or not the pattern of these cases suggests an epidemic of #some outcome of interest, using flu as a placeholder# and to do that we will need to examine them using a common epidemiological tool, an epidemiological curve. 

In order to examine the frequency of reported cases per day across the time period provided, I use the incidence package to obtain the number of cases reported each day and plot this against all of the days for which we have data.

```{r epic_DOD1, message=FALSE, echo=FALSE}
#remove notflu cases
flu1 <- filter(flu1, status == "flu")

flu1$DOD<- as.Date(flu1$DOD)
stopifnot(is.Date(flu1$DOD))

# compute daily incidence
date_case1<- incidence(flu1$DOD, interval = 1 )

epiplot1<-
  plot(date_case1, border = "white", show_cases=TRUE, 
       ylab = "Reported Cases", xlab= "Date") +
  scale_x_date(labels = date_format("%d %b %Y")) +
  theme_grey() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, color = "black")) +
  coord_fixed(ratio=2)

epiplot1
```
This pattern appears to suggest that cases increased in the beginning of JUne and end of August but since these are only from approximately 100 people, we were unsure as to how to evaluate whether these were outside of what we would expect to see within this population. 

Let's examine the 2nd set of data with around 900 people, first removing those people who are not cases, leaving us with nrows(flu2) cases over a period of approximately 1 year. 

## Daily Epidemic Curve (All Reported Cases) From 2nd Dataset {-}

```{r epic_DOD2, message=FALSE, echo=FALSE}
#remove notflu cases
flu2 <- filter(flu2, status == "flu")

flu2$DOD<- as.Date(flu2$DOD)
stopifnot(is.Date(flu2$DOD))

# compute daily incidence
date_case2<- incidence(flu2$DOD, interval = 1 )

epiplot2<-
  plot(date_case2, border = "white", show_cases=TRUE, 
       ylab = "Reported Cases", xlab= "Date") +
  scale_x_date(labels = date_format("%d %b %Y")) +
  theme_grey() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, color = "black")) +
  coord_fixed(ratio=2)

epiplot2
```
Does this suggest an unusual number of cases at any one point in time? It wasn't immediately clear to us from the picture and we couldn't compare the data to any official statistics.

As is common with data in the human rights context, it can often be difficult to obtain official statistics on incidence of a particular outcome of interest. Some group may have an interest in that data not existing in one central location or it may simply never have been collected. Either way, this requires us to break from epidemiological methods and try a classical statistical approach.

These cases are counts. This means that they can be assessed in the context of whether or not the represent a normally distributed discrete random variable in a Poisson distribution. Data which follow a normal Posisson distribution exhibit expected amounts of variation. 

In order to establish whether or not these data suggest an epidemic we construct our hypothesis to ask: are there more cases occuring than we would expect to see within the expected variation in a Poisson distribution? Specifically, on the days when we are seeing what appear to be a high number of cases per day, are these numbers statistically significantly greater than we would expect with normal variation or not? 

For this we need to account for the days on which we do not have any reported cases by adding these dates back in with counts of zero 
#how would I do this?

Next, we need to obtain the number of deaths occuring on each day or the frequency of deaths/day

In order to test our hypothesis we use the stat's package's ppois function to test the likliehood that the frequency of deaths we are observing is within the range of expected variation. Since we want to know specifically if the frequency of cases we are observing is greater than the expected number of cases we perform a one-tailed test. 

For example, if the mean number of cases reported per day is 4 and we're seeing days with 7 cases reported per day, what is the probability of having days with 7 or more cases reported per day?  

## Hypothesis testing
``` {r hypothesistest, message = FALSE}
#add in counts of zero for days on which no cases were reported

#set1
#mean number of cases/day
counts1 <- count(flu1, flu1$DOD)
mucpd1 <- mean(counts1$n)

#max number of cases/day
maxflu1 <-max(counts1$n)

#mean number of cases /day in set 2 
counts2 <- count(flu2, flu2$DOD)
mucpd2 <- mean(counts2$n)

#max number of cases/day
maxflu2 <-max(counts2$n)

#set 1
100*ppois(maxflu1, mucpd1, lower.tail=FALSE)
poisson.test(round(mucpd1), 1, round(maxflu1), alternative = "g")

#set2
100*ppois(maxflu2, mucpd2, lower.tail=FALSE)
poisson.test(round(mucpd2), 1, round(maxflu2), alternative = "g")

```
Given the mean number of cases reported per day in set one, `r mucpd1`, and the highest number of cases we see in set 1, `r maxflu1`   



## Conclusion {-}


```{r session_info, message=FALSE, include=FALSE}
sessionInfo()
```
<!---- done ---->
